{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712242a9",
   "metadata": {},
   "source": [
    "### Linear Regression with Synthetic Data\n",
    "\n",
    "In this coding exercise, you will work with synthetic data and implement linear regression using different design matrices. The goal is to calculate the beta hat estimates, y hat estimates, and residual estimates for each design matrix. You will also generate additional synthetic data with added Gaussian noise and assess the error of the regression models.\n",
    "\n",
    "Instructions:\n",
    "1. Generate a synthetic dataset (using a cosine function, a linear trend, and Gaussian noise).\n",
    "2. Define Design Matrix 1 with only the mean and linear trend.\n",
    "3. Define Design Matrix 2 by adding the cosine function to Design Matrix 1.\n",
    "4. Calculate the beta hat estimates for Design Matrix 1 and 2 using the provided formula in the slides.\n",
    "5. Calculate the y hat estimates for Design Matrix 1 and 2 using the calculated beta hat estimates.\n",
    "6. Calculate the residual estimates for Design Matrix 1 and 2 by subtracting the y hat estimates from the true values.\n",
    "7. Generate additional synthetic data with added Gaussian noise and calculate the y hat estimates for Design Matrix 1 and 2 using the new data.\n",
    "8. Assess the error by calculating the mean squared error (MSE) for Design Matrix 1 and 2, and the new data for both design matrices.\n",
    "9. Print the MSE values for Design Matrix 1 and 2, and the new data for both design matrices.\n",
    "10. Plot the true values, y hat estimates for Design Matrix 1 and 2 to visualize the linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95088aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "T = 500\n",
    "beta_0 = -0.5\n",
    "beta_1 = 0.2\n",
    "beta_2 = 0.5\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, T)\n",
    "y_true = beta_0 + beta_1*X + beta_2*np.cos(x) + np.random.normal(0, .2, T)\n",
    "\n",
    "# TODO: Visualize the data\n",
    "\n",
    "# Are we doing supervised or unsupervised learning here?\n",
    "\n",
    "# TODO: Define the design matrix for Model 1 (mean and linear trend: Model 1)\n",
    "X1 = # TODO\n",
    "\n",
    "# TODO: Define the design matrix for Model 1 (mean, linear and cosine trend: Model 2)\n",
    "X2 = # TODO\n",
    "\n",
    "\n",
    "# TODO: Calculate beta hat estimates for Model 1 and 2: b=(X'X)**(-1)X'Y?\n",
    "beta_hat_1 = # TODO\n",
    "beta_hat_2 = # TODO\n",
    "\n",
    "# Print beta estimates\n",
    "print(\"Beta hat for Model 1:\", beta_hat_1)\n",
    "print(\"Beta hat for Model 2:\", beta_hat_2)\n",
    "\n",
    "# TODO: Calculate y hat estimates for Model 1 and 2: y=xb?\n",
    "y_hat_1 = # TODO\n",
    "y_hat_2 = # TODO\n",
    "\n",
    "# TODO: Plot the y estimates for both models\n",
    "\n",
    "# TODO: Calculate residual estimates for Model 1 and 2\n",
    "residual_1 = # TODO\n",
    "residual_2 = # TODO\n",
    "\n",
    "# TODO: Calculate mean squared error (MSE: E(y - y_hat)**2) for Model 1 and 2\n",
    "mse_1 = # TODO\n",
    "mse_2 = # TODO\n",
    "\n",
    "print(\"MSE for Model 1:\", mse_1)\n",
    "print(\"MSE for Model 2:\", mse_2)\n",
    "\n",
    "\n",
    "# Generate additional synthetic data with added Gaussian noise\n",
    "x_new = np.linspace(7, 9, T//2)\n",
    "y_true_new = # TODO\n",
    "\n",
    "# TODO: Calculate y hat estimates for Model 1 and 2 with the new data\n",
    "y_hat_1_new = # TODO\n",
    "y_hat_2_new = # TODO\n",
    "\n",
    "# TODO: Calculate mean squared error (MSE: E(y - y_hat)**2) for Model 1 and 2 with the new data\n",
    "mse_1_new = # TODO\n",
    "mse_2_new = # TODO\n",
    "\n",
    "# Print MSE values\n",
    "print(\"MSE for Model 1 (New Data):\", mse_1_new)\n",
    "print(\"MSE for Model 2 (New Data):\", mse_2_new)\n",
    "\n",
    "# Which one is the training error? which one the tessting error?\n",
    "# What is the link between MSE and sigma?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0125eb",
   "metadata": {},
   "source": [
    "### Optimization Approach for Linear Regression\n",
    "\n",
    "In this coding exercise, we will explore an optimization approach for linear regression. Using a cost function and a gradient function, you will implement the gradient descent algorithm to find the optimal coefficients for each model. By completing this exercise, you will gain experience with an optimization-based approach to linear regression and understand the importance of evaluating models on both training and testing data.\n",
    "\n",
    "Your task:\n",
    "1. Implement the cost function, which calculates the mean squared error.\n",
    "2. Implement the gradient function, which computes the gradient of the cost function.\n",
    "3. Implement the gradient descent algorithm to find the optimal coefficients for both models.\n",
    "4. Calculate the y hat estimates for Model 1 and 2 using the obtained coefficients.\n",
    "5. Calculate the residuals for Model 1 and 2.\n",
    "6. Calculate the mean squared error (MSE) for Model 1 and 2.\n",
    "7. Generate additional synthetic data and calculate the y hat estimates and MSE for the new data.\n",
    "8. Interpret and compare the MSE values for both models and discuss the training and testing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the cost function (mean squared error)\n",
    "def cost_function(X, y, beta):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    assert X.shape[1] == beta.shape[0]\n",
    "    # TODO\n",
    "\n",
    "# TODO: Define the gradient function for the cost function\n",
    "def gradient_function(X, y, beta):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    assert X.shape[1] == beta.shape[0]\n",
    "    # TODO\n",
    "\n",
    "# TODO: Define the optimization algorithm (gradient descent)\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_features = X.shape[1]\n",
    "    beta = np.zeros(num_features)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        if i % 10 == 0: print(\"Minimizing cost function for lenear regression, iteration {}\".format(i))\n",
    "        grad = # TODO\n",
    "        beta -= # TODO\n",
    "        cost = # TODO\n",
    "        costs.append(cost)\n",
    "    \n",
    "    return beta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf788258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate and number of iterations for gradient descent\n",
    "learning_rate = 0.0001\n",
    "num_iterations = 100\n",
    "\n",
    "# TODO: Perform gradient descent for Model 1 and 2\n",
    "beta_hat_1, costs_1 = gradient_descent(# TODO)\n",
    "beta_hat_2, costs_2 = gradient_descent(# TODO)\n",
    "\n",
    "# Print beta estimates\n",
    "print(\"Beta hat for Model 1 (using optimization method):\", beta_hat_1)\n",
    "print(\"Beta hat for Model 2 (using optimization method):\", beta_hat_2)\n",
    "\n",
    "# TODO: Plot the cost functions evolution for both model\n",
    "\n",
    "# What are your conclusions? Did the optimization work? Did the learning stop?\n",
    "# What parameters can you change to improve the results? What are the advantages what are the limits of this approach?\n",
    "\n",
    "# TODO: Calculate y hat estimates for Model 1 and 2\n",
    "y_hat_1 = # TODO\n",
    "y_hat_2 = # TODO\n",
    "\n",
    "# TODO: Plot the data and the estimated y\n",
    "\n",
    "# TODO: Calculate residual estimates for Model 1 and 2\n",
    "residual_1 = # TODO\n",
    "residual_2 = # TODO\n",
    "\n",
    "# TODO: Calculate mean squared error (MSE: E(y - y_hat)**2) for Model 1 and 2\n",
    "mse_1 = # TODO\n",
    "mse_2 = # TODO\n",
    "\n",
    "print(\"MSE for Model 1:\", mse_1)\n",
    "print(\"MSE for Model 2:\", mse_2)\n",
    "\n",
    "# TODO: Calculate y hat estimates for Model 1 and 2 with the new data\n",
    "y_hat_1_new = # TODO\n",
    "y_hat_2_new = # TODO\n",
    "\n",
    "# TODO: Calculate mean squared error (MSE: E(y - y_hat)**2) for Model 1 and 2 with the new data\n",
    "mse_1_new = # TODO\n",
    "mse_2_new = # TODO\n",
    "\n",
    "# Print MSE values\n",
    "print(\"MSE for Model 1 (New Data):\", mse_1_new)\n",
    "print(\"MSE for Model 2 (New Data):\", mse_2_new)\n",
    "\n",
    "# What are your conclusions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
